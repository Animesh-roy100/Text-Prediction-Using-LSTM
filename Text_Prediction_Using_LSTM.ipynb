{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuYTNLrPZIfsmVO11z4HmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Animesh-roy100/Text-Prediction-Using-LSTM/blob/main/Text_Prediction_Using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCpJqzKyYTTa"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load TEXT\n",
        "#Save notepad as UTF-8 (Select from dropdown during saving)\n",
        "filename = \"/content/the_jungle_book.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "#Print first 1000 words\n",
        "print(raw_text[0:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoLTwengY3Cr",
        "outputId": "0c7eb906-d593-4518-bd40-83dafeae1967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the jungle book\n",
            "\n",
            "by rudyard kipling\n",
            "\n",
            "\n",
            "\n",
            "contents\n",
            "\n",
            "     mowgli’s brothers\n",
            "     hunting-song of the seeonee pack\n",
            "     kaa’s hunting\n",
            "     road-song of the bandar-log\n",
            "     “tiger! tiger!”\n",
            "      mowgli’s song\n",
            "     the white seal\n",
            "     lukannon\n",
            "     “rikki-tikki-tavi”\n",
            "      darzee’s chant\n",
            "     toomai of the elephants\n",
            "     shiv and the grasshopper\n",
            "     her majesty’s servants\n",
            "     parade song of the camp animals\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mowgli’s brothers\n",
            "\n",
            "     now rann the kite brings home the night\n",
            "        that mang the bat sets free--\n",
            "     the herds are shut in byre and hut\n",
            "        for loosed till dawn are we.\n",
            "     this is the hour of pride and power,\n",
            "        talon and tush and claw.\n",
            "     oh, hear the call!--good hunting all\n",
            "        that keep the jungle law!\n",
            "     night-song in the jungle\n",
            "\n",
            "it was seven o’clock of a very warm evening in the seeonee hills when\n",
            "father wolf woke up from his day’s rest, scratched himself, yawned, and\n",
            "spread out his paws one after the other to get rid of the sleepy feeling\n",
            "in their tips.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean text\n",
        "#Remove numbers\n",
        "raw_text = ''.join(c for c in raw_text if not c.isdigit())"
      ],
      "metadata": {
        "id": "89-fwsUNeLnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How many total characters do we have in our training text?\n",
        "chars = sorted(list(set(raw_text))) #List of every character\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2us-8JYGe3D-",
        "outputId": "8694d96e-9217-4724-d854-d1989dce2133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '%', '(', ')', '*', ',', '-', '.', '/', ':', ';', '?', '@', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Character sequences must be encoded as integers.\n",
        "#Each unique character will be assigned an integer value.\n",
        "#Create a dictionary of characters mapped to integer values.\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "print(char_to_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHoDZIzofS9v",
        "outputId": "05fc6245-c99c-4922-8ed2-211fd3cbc148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '%': 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, ':': 12, ';': 13, '?': 14, '@': 15, '[': 16, ']': 17, '`': 18, 'a': 19, 'b': 20, 'c': 21, 'd': 22, 'e': 23, 'f': 24, 'g': 25, 'h': 26, 'i': 27, 'j': 28, 'k': 29, 'l': 30, 'm': 31, 'n': 32, 'o': 33, 'p': 34, 'q': 35, 'r': 36, 's': 37, 't': 38, 'u': 39, 'v': 40, 'w': 41, 'x': 42, 'y': 43, 'z': 44, '‘': 45, '’': 46, '“': 47, '”': 48}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Do the reverse so we can print our predictions in characters and not interges\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(int_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qWsJb3Xf3ZQ",
        "outputId": "b8cf90ba-713c-4335-f153-ecbf85370318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '%', 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '/', 12: ':', 13: ';', 14: '?', 15: '@', 16: '[', 17: ']', 18: '`', 19: 'a', 20: 'b', 21: 'c', 22: 'd', 23: 'e', 24: 'f', 25: 'g', 26: 'h', 27: 'i', 28: 'j', 29: 'k', 30: 'l', 31: 'm', 32: 'n', 33: 'o', 34: 'p', 35: 'q', 36: 'r', 37: 's', 38: 't', 39: 'u', 40: 'v', 41: 'w', 42: 'x', 43: 'y', 44: 'z', 45: '‘', 46: '’', 47: '“', 48: '”'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#summarize the data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kypXFs8gKPf",
        "outputId": "71a10157-c5c0-462f-f52c-9c715b9111a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters in the text; corpus length:  292267\n",
            "Total Vocab:  49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 60 \n",
        "step = 10 \n",
        "sentences = [] \n",
        "next_chars = []\n",
        "for i in range(0, n_chars - seq_length, step):\n",
        "  sentences.append(raw_text[i: i+seq_length])\n",
        "  next_chars.append(raw_text[i + seq_length])\n",
        "n_patterns = len(sentences)\n",
        "print('Number of sequences:', n_patterns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD0QjNMVgze3",
        "outputId": "ad81ba98-46b8-4cda-c6ca-7ee54a3b5d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 29221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorization\n",
        "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_to_int[char]] = 1\n",
        "  y[i, char_to_int[next_chars[i]]] = 1\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "HsPhxsN9jz6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad77f4b-121b-4bb5-f1ab-adf5e09bcbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29221, 60, 49)\n",
            "(29221, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yte06YfqUWkN",
        "outputId": "6bc386ef-b221-436d-94e6-8075ee75af05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False  True False\n",
            "  False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False  True False False False False False False False False False False\n",
            "  False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False  True False False False False False False False False False\n",
            "  False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False  True False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False  True False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]\n",
            " [False  True False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]\n",
            " [False  True False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]\n",
            " [False  True False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False  True False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]\n",
            " [False  True False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model: a single LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSRtVDMVUif3",
        "outputId": "5ff231b1-f468-4f3b-fe68-c3f341e85001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               91136     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 49)                6321      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97,457\n",
            "Trainable params: 97,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "VVn_B_SaV_HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the checkpoint\n",
        "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "fG7utOZkWbYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model\n",
        "history = model.fit(x, y, batch_size=128, epochs=50, callbacks=callbacks_list)\n",
        "model.save('my_saved_weights_jungle_book_50epochs.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PDsVR--WwSg",
        "outputId": "65371c14-48ab-4077-cf8e-be90687f3112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 2.5084\n",
            "Epoch 1: loss improved from inf to 2.50838, saving model to saved_weights/saved_weights-01-2.5084.hdf5\n",
            "229/229 [==============================] - 49s 203ms/step - loss: 2.5084\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 2.0377\n",
            "Epoch 2: loss improved from 2.50838 to 2.03775, saving model to saved_weights/saved_weights-02-2.0377.hdf5\n",
            "229/229 [==============================] - 62s 269ms/step - loss: 2.0377\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.8572\n",
            "Epoch 3: loss improved from 2.03775 to 1.85722, saving model to saved_weights/saved_weights-03-1.8572.hdf5\n",
            "229/229 [==============================] - 52s 226ms/step - loss: 1.8572\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.7104\n",
            "Epoch 4: loss improved from 1.85722 to 1.71036, saving model to saved_weights/saved_weights-04-1.7104.hdf5\n",
            "229/229 [==============================] - 39s 171ms/step - loss: 1.7104\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.5946\n",
            "Epoch 5: loss improved from 1.71036 to 1.59461, saving model to saved_weights/saved_weights-05-1.5946.hdf5\n",
            "229/229 [==============================] - 47s 204ms/step - loss: 1.5946\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.4892\n",
            "Epoch 6: loss improved from 1.59461 to 1.48921, saving model to saved_weights/saved_weights-06-1.4892.hdf5\n",
            "229/229 [==============================] - 44s 192ms/step - loss: 1.4892\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.3882\n",
            "Epoch 7: loss improved from 1.48921 to 1.38821, saving model to saved_weights/saved_weights-07-1.3882.hdf5\n",
            "229/229 [==============================] - 56s 244ms/step - loss: 1.3882\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.3039\n",
            "Epoch 8: loss improved from 1.38821 to 1.30385, saving model to saved_weights/saved_weights-08-1.3039.hdf5\n",
            "229/229 [==============================] - 50s 218ms/step - loss: 1.3039\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.2373\n",
            "Epoch 9: loss improved from 1.30385 to 1.23732, saving model to saved_weights/saved_weights-09-1.2373.hdf5\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 1.2373\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.1802\n",
            "Epoch 10: loss improved from 1.23732 to 1.18024, saving model to saved_weights/saved_weights-10-1.1802.hdf5\n",
            "229/229 [==============================] - 57s 247ms/step - loss: 1.1802\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.1312\n",
            "Epoch 11: loss improved from 1.18024 to 1.13121, saving model to saved_weights/saved_weights-11-1.1312.hdf5\n",
            "229/229 [==============================] - 41s 178ms/step - loss: 1.1312\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.0918\n",
            "Epoch 12: loss improved from 1.13121 to 1.09181, saving model to saved_weights/saved_weights-12-1.0918.hdf5\n",
            "229/229 [==============================] - 38s 168ms/step - loss: 1.0918\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.0558\n",
            "Epoch 13: loss improved from 1.09181 to 1.05580, saving model to saved_weights/saved_weights-13-1.0558.hdf5\n",
            "229/229 [==============================] - 40s 176ms/step - loss: 1.0558\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.0311\n",
            "Epoch 14: loss improved from 1.05580 to 1.03115, saving model to saved_weights/saved_weights-14-1.0311.hdf5\n",
            "229/229 [==============================] - 40s 174ms/step - loss: 1.0311\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 1.0089\n",
            "Epoch 15: loss improved from 1.03115 to 1.00894, saving model to saved_weights/saved_weights-15-1.0089.hdf5\n",
            "229/229 [==============================] - 39s 168ms/step - loss: 1.0089\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.9835\n",
            "Epoch 16: loss improved from 1.00894 to 0.98351, saving model to saved_weights/saved_weights-16-0.9835.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.9835\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.9686\n",
            "Epoch 17: loss improved from 0.98351 to 0.96863, saving model to saved_weights/saved_weights-17-0.9686.hdf5\n",
            "229/229 [==============================] - 40s 174ms/step - loss: 0.9686\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.9494\n",
            "Epoch 18: loss improved from 0.96863 to 0.94942, saving model to saved_weights/saved_weights-18-0.9494.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.9494\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.9359\n",
            "Epoch 19: loss improved from 0.94942 to 0.93588, saving model to saved_weights/saved_weights-19-0.9359.hdf5\n",
            "229/229 [==============================] - 39s 168ms/step - loss: 0.9359\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.9232\n",
            "Epoch 20: loss improved from 0.93588 to 0.92322, saving model to saved_weights/saved_weights-20-0.9232.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.9232\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.9058\n",
            "Epoch 21: loss improved from 0.92322 to 0.90577, saving model to saved_weights/saved_weights-21-0.9058.hdf5\n",
            "229/229 [==============================] - 40s 174ms/step - loss: 0.9058\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8952\n",
            "Epoch 22: loss improved from 0.90577 to 0.89517, saving model to saved_weights/saved_weights-22-0.8952.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.8952\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8813\n",
            "Epoch 23: loss improved from 0.89517 to 0.88129, saving model to saved_weights/saved_weights-23-0.8813.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.8813\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8715\n",
            "Epoch 24: loss improved from 0.88129 to 0.87149, saving model to saved_weights/saved_weights-24-0.8715.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.8715\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8582\n",
            "Epoch 25: loss improved from 0.87149 to 0.85824, saving model to saved_weights/saved_weights-25-0.8582.hdf5\n",
            "229/229 [==============================] - 40s 175ms/step - loss: 0.8582\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8472\n",
            "Epoch 26: loss improved from 0.85824 to 0.84715, saving model to saved_weights/saved_weights-26-0.8472.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.8472\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8374\n",
            "Epoch 27: loss improved from 0.84715 to 0.83741, saving model to saved_weights/saved_weights-27-0.8374.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.8374\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8288\n",
            "Epoch 28: loss improved from 0.83741 to 0.82879, saving model to saved_weights/saved_weights-28-0.8288.hdf5\n",
            "229/229 [==============================] - 39s 168ms/step - loss: 0.8288\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8116\n",
            "Epoch 29: loss improved from 0.82879 to 0.81162, saving model to saved_weights/saved_weights-29-0.8116.hdf5\n",
            "229/229 [==============================] - 40s 173ms/step - loss: 0.8116\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8084\n",
            "Epoch 30: loss improved from 0.81162 to 0.80840, saving model to saved_weights/saved_weights-30-0.8084.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.8084\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.8059\n",
            "Epoch 31: loss improved from 0.80840 to 0.80594, saving model to saved_weights/saved_weights-31-0.8059.hdf5\n",
            "229/229 [==============================] - 39s 168ms/step - loss: 0.8059\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7843\n",
            "Epoch 32: loss improved from 0.80594 to 0.78430, saving model to saved_weights/saved_weights-32-0.7843.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.7843\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7754\n",
            "Epoch 33: loss improved from 0.78430 to 0.77543, saving model to saved_weights/saved_weights-33-0.7754.hdf5\n",
            "229/229 [==============================] - 40s 175ms/step - loss: 0.7754\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7690\n",
            "Epoch 34: loss improved from 0.77543 to 0.76902, saving model to saved_weights/saved_weights-34-0.7690.hdf5\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 0.7690\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7544\n",
            "Epoch 35: loss improved from 0.76902 to 0.75437, saving model to saved_weights/saved_weights-35-0.7544.hdf5\n",
            "229/229 [==============================] - 39s 168ms/step - loss: 0.7544\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7545\n",
            "Epoch 36: loss did not improve from 0.75437\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.7545\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7396\n",
            "Epoch 37: loss improved from 0.75437 to 0.73960, saving model to saved_weights/saved_weights-37-0.7396.hdf5\n",
            "229/229 [==============================] - 40s 174ms/step - loss: 0.7396\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7426\n",
            "Epoch 38: loss did not improve from 0.73960\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.7426\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7306\n",
            "Epoch 39: loss improved from 0.73960 to 0.73061, saving model to saved_weights/saved_weights-39-0.7306.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.7306\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7222\n",
            "Epoch 40: loss improved from 0.73061 to 0.72219, saving model to saved_weights/saved_weights-40-0.7222.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.7222\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7139\n",
            "Epoch 41: loss improved from 0.72219 to 0.71390, saving model to saved_weights/saved_weights-41-0.7139.hdf5\n",
            "229/229 [==============================] - 40s 174ms/step - loss: 0.7139\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.7005\n",
            "Epoch 42: loss improved from 0.71390 to 0.70053, saving model to saved_weights/saved_weights-42-0.7005.hdf5\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 0.7005\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6909\n",
            "Epoch 43: loss improved from 0.70053 to 0.69092, saving model to saved_weights/saved_weights-43-0.6909.hdf5\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 0.6909\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6891\n",
            "Epoch 44: loss improved from 0.69092 to 0.68913, saving model to saved_weights/saved_weights-44-0.6891.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.6891\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6754\n",
            "Epoch 45: loss improved from 0.68913 to 0.67537, saving model to saved_weights/saved_weights-45-0.6754.hdf5\n",
            "229/229 [==============================] - 40s 175ms/step - loss: 0.6754\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6663\n",
            "Epoch 46: loss improved from 0.67537 to 0.66625, saving model to saved_weights/saved_weights-46-0.6663.hdf5\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 0.6663\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6632\n",
            "Epoch 47: loss improved from 0.66625 to 0.66321, saving model to saved_weights/saved_weights-47-0.6632.hdf5\n",
            "229/229 [==============================] - 39s 169ms/step - loss: 0.6632\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6640\n",
            "Epoch 48: loss did not improve from 0.66321\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 0.6640\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6580\n",
            "Epoch 49: loss improved from 0.66321 to 0.65805, saving model to saved_weights/saved_weights-49-0.6580.hdf5\n",
            "229/229 [==============================] - 40s 174ms/step - loss: 0.6580\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.6496\n",
            "Epoch 50: loss improved from 0.65805 to 0.64964, saving model to saved_weights/saved_weights-50-0.6496.hdf5\n",
            "229/229 [==============================] - 39s 170ms/step - loss: 0.6496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "iB_XLpuaXETO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "epochs = range(1, len(loss)+1)\n",
        "plt.plot(epochs, loss, 'y', label = 'Training loss')\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OylMKyIzXJvo",
        "outputId": "b4903199-534c-48f8-ad31-7d52474e6de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds)\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "metadata": {
        "id": "6VC0AurAX-cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "# load the network weights\n",
        "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
        "model.load_weights(filename)"
      ],
      "metadata": {
        "id": "wzc-HoYsYgRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_index = random.randint(0, n_chars - seq_length -1)"
      ],
      "metadata": {
        "id": "4Uvo9FuiYsol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initiate generated text and keep adding new predictions and print them out\n",
        "generated = ''\n",
        "sentence = raw_text[start_index: start_index + seq_length]\n",
        "generated += sentence\n",
        "\n",
        "print('----- Seed for our text prediction: \"' + sentence + '\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AociUPDBY3HI",
        "outputId": "28549be8-d700-4907-887d-f74d0adc6517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Seed for our text prediction: \"al makes a\n",
            "slow mongoose, and if he wanted all his strength \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(400):   # Number of characters including spaces\n",
        "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
        "    for t, char in enumerate(sentence):\n",
        "        x_pred[0, t, char_to_int[char]] = 1.\n",
        "\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds)\n",
        "    next_char = int_to_char[next_index]\n",
        "\n",
        "    generated += next_char\n",
        "    sentence = sentence[1:] + next_char\n",
        "\n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4-fbe8gY9HU",
        "outputId": "e8c873ca-31fc-4a47-dbed-eb7bd7265f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e villager folvound to the great shemplying see his upfralm no, regs following; and thood and the\n",
            "for may and saw unces, looked in the madry of the onwa’s of the\n",
            "feet even shopes in his time with themorme they call, in and coulder\n",
            "for muft hear by parght ug . they i stook of the\n",
            "project gutenberg-tm wothents his mother seeld”s,\n",
            "     as they.”\n",
            "\n",
            "ake any old you read, with the dory makk flow foot\n",
            "\n",
            "ye\n"
          ]
        }
      ]
    }
  ]
}